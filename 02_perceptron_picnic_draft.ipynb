{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/practicumai.github.io/blob/main/images/icons/practicumai_beginner.png?raw=true' align='right' width=50 padding=50>\n",
    "***\n",
    "# *Practicum AI:* Deep Learning - Perceptron\n",
    "\n",
    "> This exercise adapted from the [W3 Schools Perceptrons](https://www.w3schools.com/ai/ai_perceptrons.asp) article and from Baig et al. (2020) The Deep Learning Workshop from [Packt Publishers](https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856) (Exercise 2.01, page 55).\n",
    "\n",
    " Amelia is back! This time, she's been \"asked\" to pick the next best day to host the picnic... We'll use a simple [perceptron](https://developers.google.com/machine-learning/glossary#perceptron) to predict when is the right day for the picnic based on certain conditions. As a note, this exercise lies somewhere between coding everything from scratch and relying on the pre-coded APIs (Application Programming Interfaces) that underlie the power of TensorFlow, Keras, and Pytorch. You will not need to create weight tensors beyond this exercise, but hopefully by doing it this time, you will have a better understanding (and appreciation) of the details that are often lost in an API call to `model.fit()`, for example.\n",
    "\n",
    "The table below shows some data Amelia's gathered about previous Annual Picnics. She's looking at how different factors affected faculty turnout ($y$, the output or [labels](https://developers.google.com/machine-learning/glossary#label) in our example) based on three input variables, if rain was predicted ($x_1$), if there was a guest speaker ($x_2$), and if a lakefront location was available ($x_3$). Together, $x_1$, $x_2$, and $x_3$ will be combined into our input tensor $X$.\n",
    "\n",
    "Case # | Rain Predicted? ($x_1$) | Speaker Announced? ($x_2$) | Lakefront Available? ($x_3$) | Turnout > 70% ($y$)\n",
    "--|--------------------------|---------------------|-----------------------|----------------\n",
    "1 | 1 (Yes) | 1 (Yes) | 1 (Yes) | Yes (1)\n",
    "2 | 0 (No) | 1 (Yes) | 1 (Yes) | Yes (1)\n",
    "3 | 1 (Yes) | 0 (No) | 1 (Yes) | Yes (1)\n",
    "4 | 0 (No) | 0 (No) | 1 (Yes) | Yes (1)\n",
    "5 | 1 (Yes) | 1 (Yes) | 0 (No) | Yes (1)\n",
    "6 | 0 (No) | 1 (Yes) | 0 (No) | No (0)\n",
    "7 | 1 (Yes) | 0 (No) | 0 (No) | No (0)\n",
    "8 | 0 (No) | 0 (No) | 0 (No) | No (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create an input data matrix\n",
    "\n",
    "Create a 3 x 8 matrix for our input data. Remember that we have three inputs (we'll call them $x_1$, $x_2$, and $x_3$ for now), these are the columns in our input data.\n",
    "\n",
    "The matrix below has the three input columns of our data table, using just the 0/1 values corresponding to the no/yes entries in the table. The comments help line up rows of the table with entries in our X variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "X = tf.Variable([[1.,1.,1.], # Case 1\n",
    "                 [0.,1.,1.], # Case 2\n",
    "                 [1.,0.,1.], # Case 3\n",
    "                 [0.,0.,1.], # Case 4\n",
    "                 [1.,1.,0.], # Case 5\n",
    "                 [0.,1.,0.], # Case 6\n",
    "                 [1.,0.,0.], # Case 7\n",
    "                 [0.,0.,0.]], # Case 8\n",
    "                 dtype = tf.float32)  # 3x8, input data table\n",
    "print(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a label tensor\n",
    "Create a tensor of labels to hold our 'ground truth'. This is the decision for each set of input whether or not turnout at previous events was greater than 70%. \n",
    "\n",
    "```python\n",
    "# Outputs:       1, 2, 3, 4, 5, 6, 7, 8--one for each case in the table         \n",
    "y = tf.Variable([1, 1, 1, 1, 1, 0, 0, 0], dtype = tf.float32) \n",
    "\n",
    "y = tf.reshape(y, [8,1])  \n",
    "print(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define some constants to set shape of weight matrix\n",
    "Define two constants to be used in the next step when we define the connections weight matrix.\n",
    "We can use the number of columns in the X table to determine the number of features or how many $x_i$'s we need in our model. Since we are looking for a binary decision (Yes/No) we only need one output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "NUM_FEATURES = X.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 5. Define connections weight matrix\n",
    "\n",
    "We will need one weight for each feature, $x_i$ (rain predicted, is a speaker announced, etc.), in our feature matrix, labelled $X$. These weights are our $w_i$'s. We don't know what value they should take, so we can initialize them to 0. Another common option is to use a random number to initialize the weights--this is one reason different runs of model training may give different answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "W = tf.Variable(tf.zeros([NUM_FEATURES, OUTPUT_SIZE]), dtype = tf.float32)\n",
    "print(W)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 6. Define bias variable\n",
    "\n",
    "![](images/02.1_image_6.jpg)\n",
    "\n",
    "Since we only have one neuron, we only need one bias value. Again, we'll initialize it to 0--a random number would be another option here. Again, each bias can be written as $b_i$ and the matrix of all biases as $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "B = tf.Variable(tf.zeros([OUTPUT_SIZE, 1]), dtype = tf.float32)\n",
    "print(B)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 7. Define a perceptron function\n",
    "\n",
    "In this code block, we define a perceptron function, with one input argument, X, containing our three input data features. \n",
    "\n",
    "The function's first line implements a net input function.  It multiplies the input data matrix (X) by the weights (W) using the matrix multiplication function (matmul).  It then adds the bias (B) value to that product.\n",
    "\n",
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid  #65BB7B;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> This is the essential function of a neuron: gather the inputs, multiply each input by the weight for that input, add the products up and add in the bias.</div>\n",
    "\n",
    "The function's second line implements an activation function. This determines how, if at all, the output of the neuron (calculated above) is changed before passing it on. Here we use the `tanh` activation function.  However, there are other TensorFlow options.  For example, you could use the `tf.sigmoid` function.  Or, select a function from the Keras activation (`activations`) library.  Do a search of the Keras documentation for a complete list of available functions.\n",
    "\n",
    "Try out these other options, retrain the network, and see what happens.\n",
    "\n",
    "```python\n",
    "output = tf.sigmoid(z)\n",
    "output = activations.relu(z)\n",
    "output = activations.linear(z)\n",
    "```\n",
    "\n",
    "Here is the code for the perceptron function:\n",
    "\n",
    "```python   \n",
    "def perceptron(X):\n",
    "    z = tf.add(tf.matmul(X, W), B)  # Net input function\n",
    "    output = tf.tanh(z)             # Activation function\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the perceptron function to see its initial predictions before any training.  All of its predictions ought to be 0 (remember we set all the weights and the bias to 0--so whatever the inputs are, they are all multiplied by 0 and have 0 added to the sum). \n",
    "\n",
    "```python\n",
    "print(perceptron(X))        # Execute the perceptron to see its initial predictions before training.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Training the Perceptron\n",
    "\n",
    "Now that we have the elements of a simple, single node perceptron in place, let's train the network using backpropogation. The backpropagation is implemented in the optimizer algorithm, so we don't need to code that ourselves.\n",
    "\n",
    "The [learning rate](https://developers.google.com/machine-learning/glossary#learning-rate) determines the size of the steps taken towards the global minimum while the optimizer manages the weight update process during backpropagation.  Here the Stochastic Gradient Descent (SGD) optimizer has been selected.\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Train the perceptron for 1000 epochs\n",
    "\n",
    "An [epoch](https://developers.google.com/machine-learning/glossary#epoch) is a full training pass over the entire dataset.  Our loss or error function is defined as a lambda function (a single-line,inline function) in the first line of code in the loop block.  We use the `sigmoid_cross_entropy_with_logits` function, an appropriate choice for this application to calculate how far our predicted results are from the known results. We will not get into the technical details here as that is outside the scope of this learning experience. The second line is where our SGD optimizer seeks to minimize the model's total error.\n",
    "\n",
    "```python\n",
    "no_of_epochs = 1000\n",
    "\n",
    "for n in range(no_of_epochs):\n",
    "    loss = lambda:abs(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = perceptron(X))))\n",
    "    optimizer.minimize(loss, [W, B])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Print the weights\n",
    "<img alt=\"Friendship gif\" src=\"https://i.giphy.com/media/3oEdvaba4h0I536VYQ/giphy.webp\" align=\"right\" width=\"300\">\n",
    "\n",
    "Notice that the model has learned that the faculty is awfully fond of the lakefront location! Of the weights, the 3rd one has the largest value.\n",
    "\n",
    "Given that the input from each feature will be a 0 or a 1, multiplying by a larger weight will increase the contribution of that feature in the summation of all input-by-weight products ($x_i * w_i$) in determining the output of the neuron.\n",
    "\n",
    "The perceptron has learned how to take the three input variables and weight them to predict the output. \n",
    "\n",
    "```python\n",
    "print(W)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Print the bias\n",
    "\n",
    "```python\n",
    "print(B)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Test the perceptron\n",
    "\n",
    "The numbers in the output tensor reflect the perceptron's predictions for each of the input cases. These are not probabilities, but the **model's estimate of the output value**. We could set a threshhold value and pick the next day the value is over some number to hold the picnic.\n",
    "\n",
    "```python\n",
    "print(perceptron(X))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring the X, y and predictions together to make it easier to read. Remember that `Yes=1` and `No=0` in the table.\n",
    "\n",
    "```python\n",
    "X_df = pd.DataFrame(X.numpy(), columns=['Rain Predicted?', 'Speaker Announced?', 'Lakefront Available?'])\n",
    "y_df = pd.DataFrame(y.numpy(), columns=['Turnout > 70%?'])\n",
    "pred_df = pd.DataFrame(perceptron(X).numpy(), columns=['Predictions'])\n",
    "df = pd.concat([X_df, y_df, pred_df], axis=1)\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Let's see how different choices would change the results\n",
    "\n",
    "Let's make some changes to the data and see what happens to the learned weights and predictions. \n",
    "\n",
    "##### Change 1: The faculty are extremely fond of being talked at while they eat:\n",
    "\n",
    "`y = tf.Variable([1, 1, 0, 0, 1, 0, 0, 0], dtype = tf.float32)`\n",
    "\n",
    "##### Change 2: The faculty doesn't seem to mind the rain:\n",
    "\n",
    "`y = tf.Variable([1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)`\n",
    "\n",
    "Feel free to play with other parts of the model, everying but the X inputs is replicated below to put it all in one place for easy reference. Comments point out hyperparameters that you might want to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From step 3\n",
    "# Outputs:       1, 2, 3, 4, 5, 6, 7, 8--one for each case in the table         \n",
    "y = tf.Variable([1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32) # Change 1 has been made, you'll need to make change 2\n",
    "y = tf.reshape(y, [8,1])  # convert to 4x1\n",
    "\n",
    "## From step 4\n",
    "NUM_FEATURES = X.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "## From step 5\n",
    "W = tf.Variable(tf.zeros([NUM_FEATURES, OUTPUT_SIZE]), dtype = tf.float32)\n",
    "\n",
    "## From step 6\n",
    "B = tf.Variable(tf.zeros([OUTPUT_SIZE, 1]), dtype = tf.float32)\n",
    "\n",
    "## From step 7\n",
    "def perceptron(X):\n",
    "    z = tf.add(tf.matmul(X, W), B)      \n",
    "    output = tf.tanh(z)                  # Activation function is a good hyperparameter to change \n",
    "    return output\n",
    "\n",
    "## From step 8\n",
    "learning_rate = 0.01  # Learning rate is a good hyperparameter to change\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "## From step 9\n",
    "no_of_epochs = 1000 # Number of epochs is a good hyperparameter to change\n",
    "\n",
    "for n in range(no_of_epochs):\n",
    "    loss = lambda:abs(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = perceptron(X))))\n",
    "    optimizer.minimize(loss, [W, B])\n",
    "    \n",
    "## From steps 10 on, printing the output\n",
    "print(f'Weights: {W}')\n",
    "print(f'Bias: {B}')\n",
    "\n",
    "X_df = pd.DataFrame(X.numpy(), columns=['Rain Predicted?', 'Speaker Announced?', 'Lakefront Available?'])\n",
    "y_df = pd.DataFrame(y.numpy(), columns=['Turnout > 70%?'])\n",
    "pred_df = pd.DataFrame(perceptron(X).numpy(), columns=['Predictions'])\n",
    "df = pd.concat([X_df, y_df, pred_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercises:\n",
    "\n",
    "1. Adjust some of the model's hyperparameters and see how they affect its performance.\n",
    "2. With so few inputs and samples, it's possible to get a sense of how the model would behave by just looking at the data. Add more data (features and/or samples) and see how that changes the model's behavior.\n",
    "3. Using the following code as an example, add another layer to the network:\n",
    "\n",
    "```python\n",
    "# Define the perceptron function\n",
    "def perceptron(X):\n",
    "    z1 = tf.add(tf.matmul(X, W1), B1)  # Net input function for the first layer\n",
    "    output1 = tf.tanh(z1)              # Activation function for the first layer\n",
    "    z2 = tf.add(tf.matmul(output1, W2), B2)  # Net input function for the second layer\n",
    "    output2 = tf.tanh(z2)              # Activation function for the second layer\n",
    "    return output2\n",
    "```\n",
    "NOTE: You will need to make changes throughout the notebook to reflect the new layer and it's output.\n",
    "\n",
    "4. Plot the decision boundary for the model.  This is a line that separates the two classes of data points.  \n",
    "\n",
    "Example matplotlib code for plotting the decision boundary is provided below.  You will need to modify the code to work with your model.\n",
    "\n",
    "```python\n",
    "# Plot the decision boundary\n",
    "import numpy as np\n",
    "\n",
    "x1 = np.linspace(-1, 1, 100)\n",
    "x2 = np.linspace(-1, 1, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "y_grid = perceptron(X_grid)\n",
    "y_grid = y_grid.reshape(X1.shape)\n",
    "plt.contourf(X1, X2, y_grid, cmap=plt.cm.bwr, alpha=0.2)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
