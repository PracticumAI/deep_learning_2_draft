{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/practicumai.github.io/blob/main/images/icons/practicumai_beginner.png?raw=true' align='right' width=50>\n",
    "***\n",
    "# *Practicum AI:* Deep Learning Basics\n",
    "\n",
    "\n",
    "This exercise adapted from Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercise 1.01, page 7).\n",
    "\n",
    "\n",
    "## Deep learning for image recognition\n",
    "\n",
    "\n",
    "Before we dive into the details of exactly _how_ deep learning works, let's explore it through an example. In this exercise, we will use a pre-trained deep learning model, [ResNet50](https://arxiv.org/abs/1512.03385), which has been trained on [ImageNet](https://image-net.org/), a collection of about 1.3 million images labeled as being in one of 1,000 categories.\n",
    "\n",
    "\n",
    "We'll be using image recognition to identify... Squash. Lucky for us, ImageNet is something of a squash aficionado.\n",
    "\n",
    "\n",
    "For this exercise, recall the AI Application Development Pathway:\n",
    "\n",
    "\n",
    "![Practicum AI Application Pathway Image](https://github.com/PracticumAI/deep_learning_2_draft/blob/main/AI%20Application%20Pathway.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning_2_draft/blob/main/AI%20Application%20Pathway.png' width=10 align='right' height=1>\n",
    "\n",
    "\n",
    "Step 1: Choose a Problem! Due to the flexible nature of coding, implementing the next six steps will jump around a bit. Here's the overview of the steps in the application development process and how the correspond to the code in this Jupyter Notebook:\n",
    "1. Choose a Problem - Making a squash-recognizer!\n",
    "2. Gather Good Data - We're going to \"cheat\", and use a model that's already trained to identify images with squash in them.\n",
    "3. Clean and Prep Data - This is already done for us with the pre-trained model.\n",
    "4. Choose a Model - We need a model that's already trained, and one that recognizes images. That narrows our search to models like ResNet.\n",
    "5. Train the Model - Done! We'll be up and running with an AI application without needing to compile or train anything. Magic!\n",
    "6. Evaluate the Model - As part of the evaluation process, we are going to need to test the model to see how well it recognizes squash.\n",
    "7. Deploy the Model - We're going to leave the application here in this Jupyter Notebook. Embedding the model in another application is unnecessary (and beyond the scope of this course!)\n",
    "\n",
    "\n",
    "#### 1. Import libraries\n",
    "\n",
    "\n",
    "Import the necessary libraries. For this exercise, we will use the pre-trained ResNet50 model that is part of Keras: `from tensorflow.keras.applications.resnet50 import ResNet50`. Check out the [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for image processing and deep learning. The image processing functions, like img_to_array, will help format the images to run through our model.\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "\n",
    "# Import base tensorflow and set seed to achieve consistent results.\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "seed = 42  # Set the seed for reproducibility\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate the Resnet50 model\n",
    "\n",
    "Instantiate the Resnet50 model as a variable. Instantiating is a programming term that means you're taking the 'blueprint' of something (in this case, ResNet50), and making an object out of it (the model we're going to use here). This step creates the instance of the model to use.\n",
    "\n",
    "> &#x1F4DD;  Some Background on ResNet: \n",
    "> ResNet, which stands for Residual Network, which won the 2015 ImageNet competition, was introduced to address the vanishing gradient problem commonly faced when training very deep neural networks. As networks become deeper, gradients (values used to update network weights) can become extremely small, effectively halting training. \n",
    "> ResNet introduces the concept of \"residual blocks.\"  As it processes data, instead of relying solely on the current \"thought\" or layer, it can also \"refer back\" to earlier layers, much like using recent memories to help recall older ones. These \"references back\" are called skip connections. They act like bridges, letting the network jump over some layers to ensure that even as it delves deeper into processing, it doesn't forget or lose important early details. This shortcut or skip connection allows gradients to propagate more easily through the network. \n",
    "> This architectural innovation has enabled the training of networks with depths previously thought unfeasible. ResNet models, with hundreds or even thousands of layers, have achieved state-of-the-art performance on many image classification benchmarks. \n",
    "> In this unitâ€™s exercise we used the ResNet50 model, which as its name suggests, consists of 50 layers.\n",
    "\n",
    "<div style=\"padding: 10px; margin-bottom: 20px; border: thin solid #E5C250; border-left-width: 10px;background-color: #fff\"><strong>Tip:</strong> You will likely see some output highlighted in red. While red is used for errors, it is also used for warnings. It can take some getting used to, but red is OK in this case...</div>\n",
    "\n",
    "```python\n",
    "mymodel = ResNet50() # Create an instance of the ResNet50 model pre-trained on ImageNet data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load image\n",
    "\n",
    "We'll need an image of a squash to test our model. Let's load a sqaush image in.\n",
    "\n",
    "Since ResNet50 was trained using images that are 224X224 pixels, we need to transform the input image to be the same size.\n",
    "\n",
    "<div style=\"padding: 10px; margin-bottom: 20px; border: thin solid #E5C250; border-left-width: 10px;background-color: #fff\"><strong>Tip:</strong> The squash image is stored in the images folder, the complete path of the location where the image is located must be given.\n",
    "<br><br>\n",
    "\n",
    "</div>\n",
    "\n",
    "```python\n",
    "myimage = load_img('images/squash_test.jpg', target_size = (224, 224)) # Load an image file for testing, resizing it to the required input size of 224x224 pixels\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. View the pizza image\n",
    "\n",
    "Let's take a quick look at the image to verify that it's a pizza.  Type the variable name and run the code block.\n",
    "\n",
    "```python\n",
    "myimage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Convert image to array\n",
    "\n",
    "Convert the image to an array because the model expects it in this format.\n",
    "\n",
    "```python\n",
    "myimage = img_to_array(myimage) # Convert the loaded image to an array format suitable for processing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Reshape image\n",
    "\n",
    "Reshape the image.  All images fed to this model need to be 224 pixels high and 224 pixels wide, with 3 channels, one for each color (Red, Green, Blue).  If our image was greyscale, how many channels would we specify?\n",
    "\n",
    "```python\n",
    "myimage = myimage.reshape((1, 224, 224, 3)) # Reshape the image array to the format the model expects (batch size, height, width, color channels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Pre-process image\n",
    "\n",
    "Execute the *preprocess_image()* function with the image.\n",
    "\n",
    "```python\n",
    "myimage = preprocess_input(myimage) # Preprocess the image to ensure its values are appropriate for the ResNet50 model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Execute predict method\n",
    "\n",
    "Execute the model's predict method.\n",
    "\n",
    "```python\n",
    "myresult = mymodel.predict(myimage) # Use the model to predict the class (or category) of the image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Get prediction label\n",
    "\n",
    "The model's predict method returns a number.  Convert this to its corresponding text label.\n",
    "\n",
    "```python\n",
    "mylabel = decode_predictions(myresult) # Decode the prediction result to get human-readable class labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Assign list item to variable \n",
    "\n",
    "Assign the first item listed by the prediction to a variable - this is the label with the highest probability.\n",
    "\n",
    "```python\n",
    "# Extract the label with the highest predicted probability. \n",
    "# Recalling that in Python, all indexes start at 0, he [0][0] indexing retrieves the first prediction from the first batch of results.\n",
    "mylabel = mylabel[0][0] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Embed label \n",
    "\n",
    "Embed the label in a sentence and then print it.\n",
    "\n",
    "```python\n",
    "# The 'mylabel' variable contains information about the prediction in the format (ID, Label, Probability).\n",
    "# Using 'mylabel[1]' extracts the human-readable label (e.g., 'butternut_squash') for the predicted class.\n",
    "print(\"This is an image of a \" + mylabel[1]) # Print the predicted class label in a formatted string\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #E5C250;border-left-width: 10px;background-color: #fff\"><strong>Tip:</strong> Although we use an image of a squash here, you can use just about any image with this model. Try out this exercise multiple times with different images to see if you can fool it. The <a href='https://raw.githubusercontent.com/PracticumAI/deep_learning/main/resnet_labels.txt'>resnet_labels.txt</a> file lists all the images this model is trained to classify.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Create a speech sentence\n",
    "\n",
    "Create a longer sentence to convert to speech. We want our model to output an audio file to tell us the results because, why not?\n",
    "\n",
    "```python\n",
    "sayit = \"This is an image of a \" + mylabel[1] + \" in full living color.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Import gtts libraries\n",
    "\n",
    "Import the required libraries.  Google Text to Speech (gtts) is an open source cloud-based application programming interface (API) that... Converts text to speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gTTS\n",
    "from gtts import gTTS\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Execute the gtts function\n",
    "\n",
    "Pass the sayit variable to the gTTS API.\n",
    "\n",
    "```python\n",
    "myobj = gTTS(text = sayit)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Save the audio file\n",
    "\n",
    "gTTS will convert the string you gave it into an audio file. Save the audio file. The default location is the current directory.\n",
    "\n",
    "```python\n",
    "myobj.save(\"prediction.mp3\") # Save the audio file in the current directory.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px;margin-bottom: 20px;border:  thin solid #30335D; border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> Download the .mp3 file from Atlas and listen to it on your computer. The audio file can be found in the same folder as this notebook.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Let's put it all together\n",
    "\n",
    "We can put all of these steps together in a function to make it easier to test more images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that automates the process of loading, processing, and predicting the class of an image\n",
    "def whats_this_image(image): \n",
    "    myimage = load_img(image, target_size = (224, 224))\n",
    "    myimage = img_to_array(myimage)\n",
    "    myimage = myimage.reshape((1, 224, 224, 3))\n",
    "    myimage = preprocess_input(myimage)\n",
    "    myresult = mymodel.predict(myimage)\n",
    "    mylabel = decode_predictions(myresult)\n",
    "    toplabel = mylabel[0][0]\n",
    "\n",
    "    if toplabel[1] == 'butternut_squash':\n",
    "        sayit = \"Researcher, this is a \" + toplabel[1] + \", you can breath a sigh of relief!\"\n",
    "    else:\n",
    "        sayit = \"Researcher, this is a \" + toplabel[1] + \", time to panic!\"\n",
    "\n",
    "    myobj = gTTS(text = sayit)\n",
    "\n",
    "    return mylabel, myobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, soundclip2 = whats_this_image('images/definitely_not_squash.jpg')\n",
    "print(label)\n",
    "\n",
    "soundclip2.save(\"prediction2.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should test with other images too to make sure the system is working. Find an open source image online or upload a picture of yours and give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, soundclip3 = whats_this_image('images/your_images_name.jpg')\n",
    "print(label)\n",
    "\n",
    "soundclip3.save(\"prediction3.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our classifier is working well. You can also see if you can fool the classifier with images the look like squash but aren't. Find a squash-alike image and see how the classifier does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, soundclip4 = whats_this_image('images/your_squash_alike_images_name.jpg')\n",
    "print(label)\n",
    "\n",
    "soundclip4.save(\"prediction4.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercises:\n",
    "\n",
    "1. This squash image recognizer seems to only recognize one kind of squash... Change to code to recognize each of the different kinds of squash in ImageNet.\n",
    "2. Change the code so that the squash recognizer can recognize the other kinds of squash in ImageNet as squash generally.\n",
    "3. Change the code so that if the confidence of the model's prediction is less than 75, it says it's not sure what the image is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
