{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/practicumai.github.io/blob/main/images/icons/practicumai_beginner.png?raw=true' align='right' width=50>\n",
    "***\n",
    "# *Practicum AI:* Deep Learning Basics\n",
    "\n",
    "This exercise adapted from Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercise 1.01, page 7).\n",
    "\n",
    "## Deep learning for image recognition\n",
    "\n",
    "Before we dive into the details of exactly _how_ deep learning works, let's explore it through an example. In this exercise, we will use a pre-trained deep learning model, [ResNet50](https://arxiv.org/abs/1512.03385), which has been trained on [ImageNet](https://image-net.org/), a collection of about 1.3 million images labeled as being in one of 1,000 categories. \n",
    "\n",
    "To help with this exercise, let us introduce you to Amelia. <img alt='A GAN-generated cartoon image of Amelia, our character for this exercise.' src='images/amelia.png' align='right' width=300 style=\"float: right; padding: 10px 0px 0px 10px\">\n",
    "\n",
    "Amelia, our teenaged, tech-geek heroine, happens to be an extremely picky eater. She's something of a pizza fanatic ðŸ•. When dinner time rolls around, she only wants to come down from her bedroom if pizza is on the menu. \n",
    "\n",
    "Being a tech-geek, Amelia has installed a hidden camera in the dining room.  Once dinner is set, the camera snaps a photo of the meal.  Now, she wants to develop an AI image recognition system to determine if pizza is served or not.  \n",
    "\n",
    "Let's help Amelia code her pizza recognition system!\n",
    "\n",
    "\n",
    "> &#x1F4DD; As a sidenote, the drawing of Amelia was generated by a Generative Adversarial Network (GAN) trained on Anime images and created at: [https://thisanimedoesnotexist.ai/](https://thisanimedoesnotexist.ai/). To learn more about GANs, check out our [Practicum AI GAN learning experience](https://github.com/PracticumAI/gan).\n",
    "\n",
    "Amelia is a Practicum AI alumni and recalls the AI Application Development Pathway.\n",
    "\n",
    "![Practicum AI Appliction Pathway Image](https://github.com/PracticumAI/deep_learning_2_draft/blob/main/AI%20Application%20Pathway.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning_2_draft/blob/main/AI%20Application%20Pathway.png' width=10 align='right' height=1>\n",
    "\n",
    "With her pizza-recognizing hidden camera, she's already completed Step 1: Choose a Problem! Due to the flexible nature of coding, implementing the next six steps will jump around a bit. Don't worry though, Amelia knows her stuff and will make sure we know where we're at in the application process. Here's the overiew of the steps in the application development process and how the correspond to the code in this Jupyter Notebook:\n",
    "1. Choose a Problem - Making a pizza-recognizer!\n",
    "2. Gather Good Data - Amelia is just a teenager, she can't afford to take thousands of images of pizza! Instead she's going to use a model that's already been trained to \"recognize\" pizza.\n",
    "3. Clean and Prep Data - The model she's using already has training on pizza, so she doesn't need to worry about prepping her training data. She'll have to do a bit of work to make sure that her new inputs are formatted correctly, however.\n",
    "4. Choose a Model - Amelia needs a model that's already trained (she can't go get the training data on her own) and one that recognizes images. That narrows her search to models like ResNet.\n",
    "5. Train the Model - Our heroine is going to use a model that's pre-trained so... Done! She's up and running with an AI application without needing to compile or train anything. Magic!\n",
    "6. Evalute the Model - As part of the evaluation process, Amelia is going to need to test the model to see how well it recognizes pizza.\n",
    "7. Deploy the Model - Since Amelia is comfortable using Jupyter Notebooks, she's going to leave the application here. Embedding the model in another application is uneccesary (and beyond the scope of this course!)\n",
    "\n",
    "#### 1. Import libraries\n",
    "\n",
    "Import the necessary libraries. For this exercise, Amelia will use the pre-trained ResNet50 model that is part of Keras: `from tensorflow.keras.applications.resnet50 import ResNet50`. Check out the [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for image processing and deep learning. The image processing functions, like img_to_array, will help Amelia format the image to run through her model.\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "\n",
    "# Import base tensorflow and set seed to achieve consistent results.\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "seed = 42  # Set the seed for reproducibility\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate the Resnet50 model\n",
    "\n",
    "Instantiate the Resnet50 model as a variable. Instantiating is a programming term that means you're taking the 'blueprint' of something (in this case, ResNet50), and making an object out of it (the model we're going to use here). This step creates the instance of the model to use.\n",
    "\n",
    "<div style=\"padding: 10px; margin-bottom: 20px; border: thin solid #E5C250; border-left-width: 10px;background-color: #fff\"><strong>Tip:</strong> You will likely see some output highlighted in red. While red is used for errors, it is also used for warnings. It can take some getting used to, but red is OK in this case...</div>\n",
    "\n",
    "```python\n",
    "mymodel = ResNet50() # Create an instance of the ResNet50 model pre-trained on ImageNet data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load image\n",
    "\n",
    "Amelia has a test image of her favorite pizza to use to test the system. Let's load her pizza image in.\n",
    "\n",
    "Since ResNet50 was trained using images that are 224X224 pixels, we need to transform the input image to be the same size.\n",
    "\n",
    "<div style=\"padding: 10px; margin-bottom: 20px; border: thin solid #E5C250; border-left-width: 10px;background-color: #fff\"><strong>Tip:</strong> The pizza image is stored in the images folder, the complete path of the location where the image is located must be given.\n",
    "<br><br>\n",
    "If running on Google Colab, you have a couple of choices:\n",
    "\n",
    "1. Change the `'images/pizza.jpg'` to use the web location of the image in this repository: `'https://raw.githubusercontent.com/PracticumAI/deep_learning/main/images/pizza.jpg'`\n",
    "1. Find any image of pizza, and upload it using the Files tab as in the image below. Then right click and \"Copy path\", and use that path.\n",
    "<br>\n",
    "<img src='images/colab_img_upload.png' alt='Screenshot of image upload for Google Colab'>\n",
    "    \n",
    "</div>\n",
    "\n",
    "```python\n",
    "myimage = load_img('images/pizza.jpg', target_size = (224, 224)) # Load an image file for testing, resizing it to the required input size of 224x224 pixels\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. View the pizza image\n",
    "\n",
    "Let's take a quick look at the image to verify that it's a pizza.  Type the variable name and run the code block.\n",
    "\n",
    "```python\n",
    "myimage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Convert image to array\n",
    "\n",
    "Convert the image to an array because the model expects it in this format.\n",
    "\n",
    "```python\n",
    "myimage = img_to_array(myimage) # Convert the loaded image to an array format suitable for processing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Reshape image\n",
    "\n",
    "Reshape the image.  All images fed to this model need to be 224 pixels high and 224 pixels wide, with 3 channels, one for each color (Red, Green, Blue).  If our image was greyscale, how many channels would we specify?\n",
    "\n",
    "```python\n",
    "myimage = myimage.reshape((1, 224, 224, 3)) # Reshape the image array to the format the model expects (batch size, height, width, color channels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Pre-process image\n",
    "\n",
    "Execute the *preprocess_image()* function with the image.\n",
    "\n",
    "```python\n",
    "myimage = preprocess_input(myimage) # Preprocess the image to ensure its values are appropriate for the ResNet50 model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Execute predict method\n",
    "\n",
    "Execute the model's predict method.\n",
    "\n",
    "```python\n",
    "myresult = mymodel.predict(myimage) # Use the model to predict the class (or category) of the image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Get prediction label\n",
    "\n",
    "The model's predict method returns a number.  Convert this to its corresponding text label.\n",
    "\n",
    "```python\n",
    "mylabel = decode_predictions(myresult) # Decode the prediction result to get human-readable class labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Assign list item to variable \n",
    "\n",
    "Assign the first item listed by the prediction to a variable - this is the label with the highest probability.\n",
    "\n",
    "```python\n",
    "# Extract the label with the highest predicted probability. \n",
    "# Recalling that in Python, all indexes start at 0, he [0][0] indexing retrieves the first prediction from the first batch of results.\n",
    "mylabel = mylabel[0][0] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Embed label \n",
    "\n",
    "Embed the label in a sentence and then print it.\n",
    "\n",
    "```python\n",
    "# The 'mylabel' variable contains information about the prediction in the format (ID, Label, Probability).\n",
    "# Using 'mylabel[1]' extracts the human-readable label (e.g., 'pizza') for the predicted class.\n",
    "print(\"This is an image of a \" + mylabel[1]) # Print the predicted class label in a formatted string\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #E5C250;border-left-width: 10px;background-color: #fff\"><strong>Tip:</strong> Although we use an image of a pizza here, you can use just about any image with this model. Try out this exercise multiple times with different images to see if you can fool it. The <a href='https://raw.githubusercontent.com/PracticumAI/deep_learning/main/resnet_labels.txt'>resnet_labels.txt</a> file lists all the images this model is trained to classify.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Create a speech sentence\n",
    "\n",
    "Create a longer sentence to convert to speech. Amelia wants her model to output an audio file to tell her the results so she doesn't even have to leave her beanbag chair to know what's for dinner.\n",
    "\n",
    "```python\n",
    "sayit = \"This is an image of a \" + mylabel[1] + \" in full living color.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Import gtts libraries\n",
    "\n",
    "Import the required libraries.  Google Text to Speech (gtts) is an open source cloud-based application programming interface (API) that... Converts text to speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# If you're using Google Colab, sometimes gTTS won't import. If you get an import error, use the following code (without the hashtag, of course) to install gTTS:\n",
    "# !pip install gTTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Execute the gtts function\n",
    "\n",
    "Pass the sayit variable to the gTTS API.\n",
    "\n",
    "```python\n",
    "myobj = gTTS(text = sayit)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Save the audio file\n",
    "\n",
    "gTTS will convert the string you gave it into an audio file. Save the audio file. The default location is the current directory.\n",
    "\n",
    "```python\n",
    "myobj.save(\"prediction.mp3\") # Save the audio file in the current directory.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px;margin-bottom: 20px;border:  thin solid #30335D; border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> This last block of code is only needed if you are running Jupyter Notebooks on a local computer.  Otherwise, download the .mp3 file from HiperGator and listen to it on your computer.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if running on local system as opposed to HPG or Google Colab\n",
    "# os.system(\"prediction.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Let's put it all together\n",
    "\n",
    "We can put all of these steps together in a function to make it easier to test more images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that automates the process of loading, processing, and predicting the class of an image\n",
    "def whats_for_dinner(image): \n",
    "    myimage = load_img(image, target_size = (224, 224))\n",
    "    myimage = img_to_array(myimage)\n",
    "    myimage = myimage.reshape((1, 224, 224, 3))\n",
    "    myimage = preprocess_input(myimage)\n",
    "    myresult = mymodel.predict(myimage)\n",
    "    mylabel = decode_predictions(myresult)\n",
    "    toplabel = mylabel[0][0]\n",
    "\n",
    "    if toplabel[1] == 'pizza':\n",
    "        sayit = \"Amelia, tonight's dinner is \" + toplabel[1] + \", let's go downstairs for dinner!\"\n",
    "    else:\n",
    "        sayit = \"Amelia, tonight's dinner is \" + toplabel[1] + \", let's stay here and order a pizza......!\"\n",
    "\n",
    "    myobj = gTTS(text = sayit)\n",
    "\n",
    "    return mylabel, myobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n",
      "[[('n02281787', 'lycaenid', 0.26136062), ('n03804744', 'nail', 0.11522047), ('n02277742', 'ringlet', 0.055332493), ('n07836838', 'chocolate_sauce', 0.051150553), ('n07684084', 'French_loaf', 0.034306075)]]\n"
     ]
    }
   ],
   "source": [
    "label, soundclip2 = whats_for_dinner('images/220px-Ham_(4).jpg')\n",
    "print(label)\n",
    "\n",
    "soundclip2.save(\"prediction2.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amelia should test with non-pizza meals too to make sure her system is working. Let's try this burger. ![Photo of a hamburger](images/hamburger.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, soundclip3 = whats_for_dinner('images/hamburger.jpg')\n",
    "print(label)\n",
    "\n",
    "soundclip3.save(\"prediction3.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like Amelia's classifier is working well as it predicted that the image was a cheeseburger. \n",
    "\n",
    "But Amelia's parents are on to her...they started making food that looks like pizza, but isn't! How about this quiche? Does it fool the AI? Can you find images that trick Amelia into coming down for dinner?\n",
    "\n",
    "![A photo of quiche](images/quiche.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, soundclip4 = whats_for_dinner('images/quiche.jpg')\n",
    "print(label)\n",
    "\n",
    "soundclip4.save(\"prediction4.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
